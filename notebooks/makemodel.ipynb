{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from os.path import basename\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_labels = ['track_length_since_start', 'time_since_first_station', 'station_number', 'lat', 'lon', 'track_length', 'zeit']\n",
    "X = df[feat_labels]\n",
    "y = df['isadelay5']\n",
    "print('SplitDataset')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datei_info = {}\n",
    "df['daytype']\n",
    "daytype_cats = {'Weekend': 0,  'Weekday': 1}\n",
    "df['daytype_cat'] = df['daytype'].map(daytype_cats)\n",
    "df['type_cat'] = pd.Categorical(df['type'], categories=df['type'].unique()).codes\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ZeroR')\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_classifier.fit( X_train,y_train)\n",
    "y_dummy_classifier = dummy_classifier.predict(X_test)\n",
    "datei_info['zeror'] = 1 - ((y_test != y_dummy_classifier).sum() / X_test.shape[0])\n",
    "datei_info['zeror_matrix'] = confusion_matrix(y_test, y_dummy_classifier, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DecisionTree')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "datei_info['dt'] = 1 - ((y_test != y_pred_dt).sum() / X_test.shape[0])\n",
    "datei_info['dt_matrix'] = confusion_matrix(y_test, y_pred_dt, labels=[0,1])\n",
    "print(classification_report(y_test,y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RandomForest')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=7,n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_clf=clf.predict(X_test)\n",
    "datei_info['clf'] = 1 - ((y_test != y_pred_clf).sum() / X_test.shape[0])\n",
    "datei_info['clf_matrix'] = confusion_matrix(y_test, y_pred_clf, labels=[0,1])\n",
    "print(classification_report(y_test,y_pred_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = df.dropna()\n",
    "#feat_labelsnew = ['zeit',\n",
    "#     'temperature_c', 'air_pressure_hpa',\n",
    " #      'relative_humidity', 'dew_point_c', 'wind_speed_kmh',\n",
    "  #      'type_cat', 'time_since_last_station',\n",
    "   #    'time_since_first_station', 'stay_time', 'track_length',\n",
    "    #   'track_length_since_start', 'station_number', 'lon', 'lat', 'bhf_cat',\n",
    "     #  'dayname_cat', 'trainno_cat', 'weather_condition_cat', 'daytype_cat']\n",
    "feat_labelsnew = ['zeit', 'track_length_since_start', 'time_since_first_station','time_since_last_station', 'station_number', 'bhf_cat', 'trainno_cat']\n",
    "X = dfnew[feat_labelsnew]\n",
    "y = dfnew['isadelay5']\n",
    "print(X.count())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print('RandomForestnew')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clfnew=ExtraTreesClassifier(n_estimators=len(feat_labelsnew)-2,n_jobs=-1,random_state=0)\n",
    "clfnew.fit(X_train,y_train)\n",
    "y_pred_clfnew=clfnew.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datei_info['clfnew'] = 1 - ((y_test != (y_pred_clfnew > 0.50)).sum() / X_test.shape[0])\n",
    "print(y_pred_clfnew)\n",
    "datei_info['clfnew_matrix'] = confusion_matrix(y_test, (y_pred_clfnew>0.50), labels=[0,1])\n",
    "print(classification_report(y_test,(y_pred_clfnew > 0.5)))\n",
    "print(datei_info)\n",
    "for feature in zip(feat_labelsnew, clfnew.feature_importances_):\n",
    "    print(feature)\n",
    "print(avg(y_pred_clfnew[y_test != (y_pred_clfnew > 0.5)]))\n",
    "plt.plot(y_pred_clfnew[(y_test != (y_pred_clfnew > 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "# pickle.dump(clfnew, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "clfnew = pickle.load(open(filename, 'rb'))\n",
    "clfnew.predict_proba([[8.0, 204.0, 3.0, 0.0, 0.0, 4.0, 4.0, 22.0]])\n",
    "\n",
    "#datei_info['clfnew'] = 1 - ((y_test.iloc[0] != y_pred_clfnew).sum() / X_test.shape[0])\n",
    "#datei_info['clfnew_matrix'] = confusion_matrix(y_test, y_pred_clfnew, labels=[0,1])\n",
    "#print(classification_report(y_test,y_pred_clfnew))\n",
    "#print(datei_info)\n",
    "#feat_labelsnew.remove('isadelay5')\n",
    "#for feature in zip(feat_labelsnew, clfnew.feature_importances_):\n",
    "    #print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.iloc[0].to_list())\n",
    "print(y_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datei_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# # Create a selector object that will use the random forest classifier to identify\n",
    "# # features that have an importance of more than 0.15\n",
    "sfm = SelectFromModel(clf, threshold=0.02)\n",
    "\n",
    "# Train the selector\n",
    "sfm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the most important features\n",
    "# for feature_list_index in sfm.get_support(indices=True):\n",
    "#     print(feat_labels[feature_list_index])\n",
    "for feature in zip(feat_labels, clf.feature_importances_):\n",
    "    print(feature)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a selector object that will use the random forest classifier to identify\n",
    "# features that have an importance of more than 0.15\n",
    "#feat_labelsnew.remove('isadelay5')\n",
    "sfmnew = SelectFromModel(clfnew, threshold=0.02)\n",
    "# Train the selector\n",
    "sfmnew.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the names of the most important features\n",
    "for feature_list_index in sfmnew.get_support(indices=True):\n",
    "    print(feat_labelsnew[feature_list_index])\n",
    "\n",
    "for feature in zip(feat_labelsnew, clfnew.feature_importances_):\n",
    "    print(feature)\n",
    "clfnew=RandomForestClassifier(n_estimators=len(feat_labelsnew)-2,n_jobs=-1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = [{'startbhf': 'Tübingen Hbf', 'endbhf': 'Köln Hbf', 'starttrain': 'RE_22032', 'startplatform': '2', 'endplatform': '5', 'starttime': '13:37', 'endtime': '17:05'}, {'anbhf': 'Stuttgart Hbf', 'abbhf': 'Stuttgart Hbf', 'antrain': 'RE_22032', 'abtrain': 'ICE_2536', 'transfertime': 13, 'anplatform': '2', 'abplatform': '2', 'anzeit': '14:38', 'abzeit': '14:51'}, {'anbhf': 'Mannheim Hbf', 'abbhf': 'Mannheim Hbf', 'antrain': 'ICE_2536', 'abtrain': 'ICE_106', 'transfertime': 6, 'anplatform': '3', 'abplatform': '3', 'anzeit': '15:31', 'abzeit': '15:35'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = connection[1]\n",
    "import json\n",
    "with open('newData/conversion_dicts.json') as json_file:\n",
    "    conversion_dicts = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con['abtrain'].startswith('IC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(connection[2]['abtrain'])\n",
    "if connection[2]['abtrain'] in conversion_dicts['trainno_cats']:\n",
    "  print(\"blah\")\n",
    "else:\n",
    "  print(\"boo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1,len(connection)):\n",
    "    data = {'zeit': 0 , 'track_length_since_start': 0 , 'time_since_first_station': 0 ,'time_since_last_station': 0 , 'station_number': 0 , 'bhf_cat': 0 , 'trainno_cat': 0 }\n",
    "    con = connection[x]\n",
    "    if con['antrain'] in conversion_dicts['trainno_cats']:\n",
    "        if int(con['anzeit'].replace(':', ''))%100 > 30: #We dont need minutes so we form %H:%M to ~%H\n",
    "            data['zeit'] = round(int(con['anzeit'].replace(':', ''))/100) + 1\n",
    "        else:\n",
    "            data['zeit'] = round(int(con['anzeit'].replace(':', ''))/100)\n",
    "        fahrplan = pd.read_csv('newData/'+con['antrain']+'.csv', index_col=False)\n",
    "        fahrplan = fahrplan.set_index('bhf')\n",
    "        data['track_length_since_start'] = fahrplan.at[con['anbhf'],'track_length_since_start'][0]\n",
    "        data['time_since_last_station'] = fahrplan.at[con['anbhf'],'time_since_last_station'][0]\n",
    "        data['time_since_first_station'] = fahrplan.at[con['anbhf'],'time_since_first_station'][0]\n",
    "        data['station_number'] = fahrplan.at[con['anbhf'],'station_number'][0]\n",
    "        data['bhf_cat'] = conversion_dicts['bhf_cats'][con['anbhf']]\n",
    "        data['trainno_cat'] = conversion_dicts['trainno_cats'][con['antrain']]\n",
    "        print(data)\n",
    "        predict(model, list(data.values()))\n",
    "    else:\n",
    "        print(\"Train not in Database\")\n",
    "    if con['abtrain'] in conversion_dicts['trainno_cats']:\n",
    "        if int(con['abzeit'].replace(':', ''))%100 > 30: #We dont need minutes so we form %H:%M to ~%H\n",
    "            data['zeit'] = round(int(con['abzeit'].replace(':', ''))/100) + 1\n",
    "        else:\n",
    "            data['zeit'] = round(int(con['abzeit'].replace(':', ''))/100)\n",
    "        fahrplan = pd.read_csv('newData/'+con['abtrain']+'.csv', index_col=False)\n",
    "        fahrplan = fahrplan.set_index('bhf')\n",
    "        data['track_length_since_start'] = fahrplan.at[con['abbhf'],'track_length_since_start'][0]\n",
    "        data['time_since_last_station'] = fahrplan.at[con['abbhf'],'time_since_last_station'][0]\n",
    "        data['time_since_first_station'] = fahrplan.at[con['abbhf'],'time_since_first_station'][0]\n",
    "        data['station_number'] = fahrplan.at[con['abbhf'],'station_number'][0]\n",
    "        print(con['abbhf'])\n",
    "        data['bhf_cat'] = conversion_dicts['bhf_cats'][con['abbhf']]\n",
    "        data['trainno_cat'] = conversion_dicts['trainno_cats'][con['abtrain']]\n",
    "        print(data)\n",
    "        predict(model, list(data.values()))\n",
    "    else:\n",
    "        print(\"Train not in Database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrplan = pd.read_csv('newData/ICE_106.csv', index_col=False)\n",
    "fahrplan = fahrplan.set_index('bhf')\n",
    "#print(fahrplan.head())\n",
    "fahrplan.at['Mannheim Hbf','track_length_since_start'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}