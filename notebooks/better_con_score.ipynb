{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbc34dfe4857b4389ab4dceed4ab98cc1",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# self-writen stuff\n",
    "from predict_data import prediction_data\n",
    "from random_forest import predictor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../better_con_score.csv'\n",
    "combined = pd.read_csv(path, engine='c', compression='zip')\n",
    "combined = combined[combined['adelay0'] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = combined[['transtime', 'adelay0', 'adelay5', 'adelay10', 'adelay15', 'ddelay0', 'ddelay5', 'ddelay10', 'ddelay15', 'doable3']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_con_score(transfertime, pred1, pred2):\n",
    "    con_score = np.zeros(len(pred1))\n",
    "    print(len(pred1))\n",
    "    if (transfertime > 17):\n",
    "        con_score = 1 #if the transfer time is higher than our highest lable, we can only predict the connection as working\n",
    "\n",
    "    elif (transfertime > 12):\n",
    "        p1 = pred1[:, 3] * (1 - pred2[:, 3])\n",
    "        con_score = 1 - (p1)\n",
    "\n",
    "    elif (transfertime > 7):\n",
    "        #if the arrival train has 10 min delay, and the departure one does not have 5 min delay\n",
    "        p1 = (pred1[:, 2] - pred1[:, 3]) * (1 - pred2[:, 1])\n",
    "\n",
    "        #if the arrival train has 15 min delay, and the departure one does not have 10 min delay\n",
    "        p2 = pred1[:, 3] * (1 - pred2[:, 2])\n",
    "        con_score = 1 - (p1+p2)\n",
    "\n",
    "    else:\n",
    "        p1 = (pred1[:, 1] - pred1[:, 2]) * (1 - pred2[:, 1])\n",
    "        p2 = (pred1[:, 2] - pred1[:, 3]) * (1 - pred2[:, 2])\n",
    "        p3 = pred1[:, 3] * (1 - pred2[:, 3])\n",
    "        con_score = 1 - (p1+p2+p3)\n",
    "    \n",
    "    return con_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X, y, X_test, y_test, name='better_con_score'):\n",
    "    clf = ExtraTreesClassifier(n_estimators=20, max_depth=5,\n",
    "                               random_state=0, n_jobs = 4, criterion = 'entropy')\n",
    "    # LogisticRegression(random_state=0)\n",
    "    clf.fit(X.astype('float'), y.astype('bool'))\n",
    "\n",
    "    #make and print some acc statistics\n",
    "    test(X_test, y_test, clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test, y_test, clf):\n",
    "    test_pred = clf.predict_proba(X_test)\n",
    "    evaluate_test(test_pred[:, 0], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(test_pred, y_test):\n",
    "    bool_test_pred = test_pred > 0.5\n",
    "    print(bool_test_pred)\n",
    "    print(y_test)\n",
    "    print(bool_test_pred == y_test)\n",
    "    \n",
    "    results = {}\n",
    "    results['acc'] = len(test_pred[bool_test_pred == y_test]) / len(test_pred)\n",
    "    for i in range(10):\n",
    "        trues = len(test_pred[(test_pred > (i/10)) & (test_pred < (i/10+0.1)) & (bool_test_pred == y_test)])\n",
    "        complete = len(test_pred[(test_pred > (i/10)) & (test_pred < (i/10+0.1))])\n",
    "        print(trues, '/', complete)\n",
    "        try:\n",
    "            results['acc'+ str(i)] = trues / complete\n",
    "        except ZeroDivisionError:\n",
    "            results['acc'+ str(i)] = np.nan\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sets(dataset):\n",
    "    np.random.shuffle(dataset)\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(dataset[:, :9], dataset[:, -1], test_size=0.10, random_state=42)\n",
    "\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, x, Y, y = np_sets(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_score = calc_con_score(15, dataset[(dataset[:, 0] > 12).astype('bool') & (dataset[:, 0] < 18).astype('bool'), 1:5], dataset[(dataset[:, 0] > 12).astype('bool') & (dataset[:, 0] < 18).astype('bool'), 5:9])\n",
    "dataset[(dataset[:, 0] > 12).astype('bool')  & (dataset[:, 0] < 18).astype('bool') , -1] = con_score\n",
    "# dataset[dataset[:, 0] > 17, -1] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = dataset[(dataset[:, 0] > 12).astype('bool') & (dataset[:, 0] < 18).astype('bool'), -1] == True\n",
    "evaluate_test(con_score, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test(X, Y, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined[combined['doable3'] == True]) / len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((dataset.shape[0],dataset.shape[1]+1))\n",
    "b[:,:-1] = dataset\n",
    "dataset = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = dataset[(dataset[:, -1] > 0.5).astype('bool') == dataset[:, -2]]\n",
    "# print('Feature importances', clf.feature_importances_)\n",
    "print('acc_' + 'name' + ':', len(acc)/ len(dataset))"
   ]
  }
 ]
}