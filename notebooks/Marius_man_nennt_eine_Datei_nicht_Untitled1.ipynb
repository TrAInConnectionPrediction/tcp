{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from os.path import basename\n",
    "\n",
    "path = r'trainData/' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    if not basename(filename).startswith('I'):\n",
    "        continue\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    if df['daytype'].unique()[0] == 0:\n",
    "        print(filename)\n",
    "        df.loc[df['daytype'].isin([0]), 'daytype'] = 'Weekday'\n",
    "        df.loc[df['daytype'].isin([1]), 'daytype'] = 'Weekend'\n",
    "        \n",
    "    li.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.concat(li, axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['daytype'].value_counts()\n",
    "frame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_csv('newData/NeueICEs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wetterdaten/bahnhof_to_weather_location.csv', index_col=None, header=0)\n",
    "bundesland = []\n",
    "land = []\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stadt in df['location']:\n",
    "    if stadt != 'no-data':\n",
    "        payload = {'placename': stadt.replace('+', ' '),'maxRows':'1', 'username': 'bwki'}\n",
    "        #r = requests.get('http://api.geonames.org/postalCodeSearchJSON', params=payload)\n",
    "        try:\n",
    "            r = requests.get('http://api.geonames.org/postalCodeSearchJSON',  params=payload)\n",
    "            if (r.status_code != 200 or r.text == '{\"postalCodes\":[]}'):\n",
    "                bundesland.append('BY')\n",
    "                print(r.url)\n",
    "                print('Bundesland = BY')\n",
    "                continue\n",
    "            lol = r.json()\n",
    "        except requests.exceptions.ChunkedEncodingError:\n",
    "            print('retrying...')\n",
    "            continue\n",
    "        print(r.url)\n",
    "        print(lol['postalCodes'][0]['ISO3166-2'])\n",
    "        #lol = r.json()\n",
    "        bundesland.append(lol['postalCodes'][0]['ISO3166-2'])\n",
    "    else:\n",
    "        bundesland.append('BY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['land'] = bundesland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('bhf').loc['Leipzig Halle Flughafen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('wetterdaten/bahnhof_to_weather_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import sys\n",
    "\n",
    "frame = pd.read_csv('../data/combinedData/alltrains2019.csv',encoding='UTF-8',compression='zip', engine='c')\n",
    "bhfs = pd.read_csv('../stations.csv',encoding='UTF-8')\n",
    "bhfs = bhfs[bhfs.loc[:,'filename'].notnull()].set_index(\"filename\")\n",
    "frame = frame.set_index(\"bhf\")\n",
    "\n",
    "filename = 'Halle.json'\n",
    "with open('../data/wetterdaten/wetterdaten2019/' + filename) as json_file:\n",
    "    weather_json = pd.DataFrame.from_dict(json.load(json_file)['history_1h'])\n",
    "try:\n",
    "    bhf = frame.loc[frame.index.isin(bhfs.loc[filename.replace(\".json\", \"\"), \"name\"].tolist())].reset_index()\n",
    "except:\n",
    "    print(str(sys.exc_info()[0]) + \" Error \" + bhfs.loc[filename.replace(\".json\", \"\"), \"name\"])\n",
    "    bhf = frame.loc[frame2.index.isin([bhfs.loc[filename.replace(\".json\", \"\"), \"name\"]])].reset_index()\n",
    "\n",
    "bhf = bhf.reset_index()\n",
    "data = pd.DataFrame()\n",
    "if len(bhf) < 1:\n",
    "    exit()\n",
    "#weather_json.set_index(\"time\", inplace = True)\n",
    "for i, x in bhf.iterrows():\n",
    "    date = datetime.strptime(bhf.at[i,'date'], '%Y-%m-%d')\n",
    "    ####-Add-weather-to-trainData-####\n",
    "\n",
    "    # 1. nÃ¤chste Uhrzeit finden\n",
    "    arr_time = bhf.at[i, 'arr'] if bhf.at[i, 'arr'] != '99:99' else bhf.at[i, 'dep']\n",
    "    try: #sometimes there are times like '14:99'. We are skipping those.\n",
    "        arr_time = pd.to_datetime(date.strftime('%Y-%m-%d') + 'T' + arr_time)\n",
    "    except:\n",
    "        print(\"except\")\n",
    "        continue\n",
    "    x = weather_json['time'].astype('datetime64[h]').reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    #Calculate the timedelta amount\n",
    "    v = x.apply(lambda dep_time: dep_time - arr_time if dep_time > arr_time else arr_time - dep_time)\n",
    "\n",
    "\n",
    "    #find nearest time\n",
    "    date = x[v.idxmin()].strftime('%Y-%m-%d %H:%M')\n",
    "    #print(date)\n",
    "    #all data with matching date and time\n",
    "    data =  weather_json.loc[weather_json['time'].isin([date]), :] #.astype('datetime64[ns]')\n",
    "    data = data.reset_index(drop=True)\n",
    "    # 2. daten eintragen\n",
    "    bhf.at[i, 'sealevelpressure'] = data.at[0, 'sealevelpressure']\n",
    "    bhf.at[i, 'temperature'] = data.at[0, 'temperature']\n",
    "    bhf.at[i, 'precipitation'] = data.at[0, 'precipitation']\n",
    "    bhf.at[i, 'snowfraction'] = data.at[0, 'snowfraction']\n",
    "    bhf.at[i, 'winddirection'] = data.at[0, 'winddirection']\n",
    "    bhf.at[i, 'windspeed'] = data.at[0, 'windspeed']\n",
    "    bhf.at[i, 'relativehumidity'] = data.at[0, 'relativehumidity']\n",
    "    ####--------------------------####\n",
    "    if ((i % 100) == 0):\n",
    "        print('|', end='')\n",
    "bhf.to_csv('../data/wetterdaten/combined/' + filename.replace(\".json\", \"\") + '.csv',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhfs = pd.read_csv('../stations.csv',encoding='UTF-8')\n",
    "bhfs2 = bhfs.set_index(\"city_search_string\")\n",
    "path = '../data/wetterdaten/wetterdaten2019/'\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if not filename.replace(\".json\", \"\")  in bhfs2['filename'].tolist():\n",
    "        os.system('python3 ../add_data.py ' + filename +' >& /dev/null &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}