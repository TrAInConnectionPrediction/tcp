{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X, y, X_test, y_test, model, save = False):\n",
    "    start_time = time.time()\n",
    "    #fit model\n",
    "    model.fit(X, y)\n",
    "    #make and print some acc statistics\n",
    "    test_pred = model.predict(X_test)\n",
    "      \n",
    "    acc = test_pred[test_pred == y_test]\n",
    "    \n",
    "    print(1 - ((y_test != test_pred).sum() / X_test.shape[0]))\n",
    "    print('Feature importances', model.feature_importances_)\n",
    "    print('acc: ' + str(len(acc)/ len(test_pred)))\n",
    "    print(accuracy_score(y_test, test_pred))\n",
    "    #print(\"Confusion Matrix: \" + str(confusion_matrix(y_test, test_pred, labels=[0,1])))\n",
    "    print('Confusion Matrix:', multilabel_confusion_matrix(y_test, test_pred))\n",
    "    if save:\n",
    "        dump(model, '../server/test_rf.joblib')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dm = DummyClassifier(strategy=\"most_frequent\")\n",
    "dm.fit(X, y)\n",
    "test_pred = dm.predict(X_test)\n",
    "acc = test_pred[test_pred == y_test]\n",
    "    \n",
    "print(\"BASEÂ§LINE: \"+ str(len(acc)/ len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model_dt = ExtraTreesClassifier(n_estimators=70, max_depth=12,\n",
    "                                 random_state=0, n_jobs = -1)\n",
    "train_and_test(X, y.ravel(), X_test, y_test.ravel(), model_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_dt = RandomForestClassifier(n_estimators=70, max_depth=12,\n",
    "                                 random_state=0, n_jobs = -1)\n",
    "train_and_test(X, y, X_test, y_test, model_dt, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here on automatic models for poster"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''Change Size of jupyter notebook'''\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  '../weather.csv'\n",
    "dataset = pd.read_csv(path, index_col=False, compression='zip', engine='c')\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dates"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.drop(['temperature_c', 'air_pressure_hpa', 'relative_humidity', 'dew_point_c', 'wind_speed_kmh', 'weather_condition'], inplace = True, axis=1)\n",
    "date = dataset['date'].astype('datetime64[D]')\n",
    "dataset['month'] = date.dt.month\n",
    "dataset['dayofweek'] = date.dt.dayofweek\n",
    "dataset['hour'] = dataset['zeit']\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(dset, label, random_state):\n",
    "    # make the balance of delayed trains in the dataset better\n",
    "    #split dataset\n",
    "    minor = dset[dataset[label] == True] \n",
    "    major = dset[dataset[label] == False]\n",
    "    #set major dataset to lenght of minor dataset by randomly seletcting datapoints\n",
    "    major = major.sample(n=len(minor),random_state=random_state)\n",
    "    #combine datsets\n",
    "    balancedset = pd.concat([minor, major], ignore_index=True, sort=False)\n",
    "    #I think this shuffels? and ensure length\n",
    "    balancedset = balancedset.sample(n=len(balancedset),random_state=random_state).reset_index(drop=True)\n",
    "    #print(len(balancedset))\n",
    "    return balancedset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = random.randint(1, 1000)\n",
    "label = 'isadelay'\n",
    "feat_labels = [ 'month',\n",
    "                'dayofweek',\n",
    "                'hour',\n",
    "                'time_since_first_station',\n",
    "                'station_number',\n",
    "                'lat',\n",
    "                'lon',\n",
    "                'stay_time',\n",
    "                'time_since_last_station',\n",
    "                'total_time',\n",
    "                'delta_lon',\n",
    "                'delta_lat',\n",
    "                  ]\n",
    "                # 'relative_humidity', 'dew_point_c', 'air_pressure_hpa', 'temperature_c', 'trainno', 'weather_condition', 'type', 'bhf', 'wind_speed_kmh',\n",
    "df = dataset.sample(frac=1,random_state=random_state)\n",
    "X = dataset[feat_labels]\n",
    "y = dataset[label]\n",
    "del df\n",
    "print('SplitDataset')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "#and now with balanced\n",
    "dataset_bal = balance(dataset, label, random_state)\n",
    "X_bal = dataset_bal[feat_labels]\n",
    "y_bal = dataset_bal[label]\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_bal, y_bal, test_size=0.1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Held out something"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def held_out_label(df, feat_labels, label, held_out,held_out_var):\n",
    "    X = df[feat_labels]\n",
    "    y = df[label]\n",
    "    X_train = df[df[held_out_var] != held_out][feat_labels]\n",
    "    X_test = df[df[held_out_var] == held_out][feat_labels]\n",
    "    y_train = df[df[held_out_var] != held_out][label]\n",
    "    y_test = df[df[held_out_var] == held_out][label]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51.517899 = Dortmund Hbf\n",
    "held_out = 51.517899\n",
    "held_out_var = 'lat'\n",
    "X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, held_out, held_out_var)\n",
    "X_train_bal, X_test_bal, y_train_bal,y_test_bal = held_out_label(dataset_bal, feat_labels, label, held_out, held_out_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_auc(actual, preds):\n",
    "    fpr, tpr, thresholds = roc_curve(actual, preds[:,1])\n",
    "    plt.plot(fpr, tpr\n",
    "             ,'r')\n",
    "    plt.plot([0,1],[0,1],'b')\n",
    "    plt.title('AUC: {}'.format(auc(fpr,tpr)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "'''Define your models here'''\n",
    "models = {}\n",
    "#models['RandomForest'] = RandomForestClassifier(n_estimators=len(feat_labels)-2, max_depth=12, n_jobs=-1,random_state=random_state),\n",
    "#models['DecisionTree'] = DecisionTreeClassifier(max_depth = 12, random_state=random_state),\n",
    "models['RandomForest'] = RandomForestClassifier(n_estimators=128, max_depth = 12, n_jobs=-1,random_state=random_state)\n",
    "models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=128,max_depth = 12, n_jobs=-1,random_state=random_state)\n",
    "\n",
    "'''Calculate Baseline Infos'''\n",
    "zeroR = DummyClassifier(strategy=\"most_frequent\").fit( X_train,y_train).predict(X_test)\n",
    "zeroR_bal = DummyClassifier(strategy=\"most_frequent\").fit( X_train_bal,y_train_bal).predict(X_test_bal)\n",
    "infos = {'random_state': random_state,\n",
    "         'ZeroR': (1 - ((y_test != zeroR).sum() / X_test.shape[0])),\n",
    "         'ZeroR_bal': (1 - ((y_test_bal != zeroR_bal).sum() / X_test_bal.shape[0]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    \n",
    "    if False:\n",
    "        scores = cross_val_score(models[model], X, y, cv=StratifiedKFold(n_splits=5))\n",
    "        infos[model + \"_cross_val_strat\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "        \n",
    "    scores = cross_val_score(models[model], X, y, cv=5)    \n",
    "    infos[model + \"_cross_val\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "    \n",
    "    models[model].fit(X_train ,y_train)\n",
    "    model_classified = models[model].predict(X_test)\n",
    "    infos[model] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "    \n",
    "    infos[model + '_matrix'] = confusion_matrix(y_test, model_classified, labels=[0,1])\n",
    "    \n",
    "\n",
    "    '''Balanced'''\n",
    "    \n",
    "    if False:\n",
    "        scores = cross_val_score(models[model], X_bal, y_bal, cv=StratifiedKFold(n_splits=5))\n",
    "        infos[model + \"_cross_val_strat_bal\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "        \n",
    "        scores = cross_val_score(models[model], X_bal, y_bal, cv=5)    \n",
    "        infos[model + \"_cross_val_bal\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "    \n",
    "        models[model].fit(X_train_bal ,y_train_bal)\n",
    "        model_classified = models[model].predict(X_test_bal)\n",
    "        infos[model + \"_bal\"] = 1 - ((y_test_bal != model_classified).sum() / X_test_bal.shape[0])\n",
    "\n",
    "        infos[model + '_matrix_bal'] = confusion_matrix(y_test_bal, model_classified, labels=[0,1])\n",
    "    \n",
    "    #for feature in zip(feat_labels, models[model].feature_importances_):\n",
    "    #   print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(y_test, DummyClassifier(strategy=\"most_frequent\").fit( X_train,y_train).predict_proba(X_test))\n",
    "plot_roc_auc(y_test, models['RandomForest'].predict_proba(X_test))\n",
    "plot_roc_auc(y_test, models['ExtraTrees'].predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['train_acc']={}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for train in dataset['trainno'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'trainno')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['train_acc'][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "    \n",
    "    #infos[train + '_matrix'] = confusion_matrix(y_test, model_classified, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['train_acc']={}\n",
    "train_base={}\n",
    "for train in dataset['trainno'].unique():\n",
    "    print(train,end=', ')\n",
    "    test = df[df[held_out_var] == held_out]\n",
    "    train_base[train] = len(test[test['isadelay5'] == True]) / len(test[test['isadelay5'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(train_acc.values())) / len(list(train_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(train_base.values())) / len(list(train_base.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['trainno'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(list(train_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['bhf_acc']={}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for train in dataset['bhf'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'bhf')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['bhf_acc'][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(bhf_acc.values())) / len(list(bhf_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(list(bhf_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(frac=1,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# udfs ----\n",
    "\n",
    "# function for creating a feature importance dataframe\n",
    "def imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title):\n",
    "    imp_df.columns = ['feature', 'feature_importance']\n",
    "    a4_dims = (11.7, 6)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    sns.barplot(ax=ax ,x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n",
    "       .set_title(title, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {'month': 'Monat', 'dayofweek': 'Wochentag', 'hour': 'Uhrzeit', 'time_since_first_station': 'Gesamte Fahrzeit', 'station_number': 'Anzahl Halte',\n",
    "                                  'lat': 'Breitengrad', 'lon': 'LÃ¤ngengrad', 'lon': 'LÃ¤ngengrad', 'stay_time': 'Aufenthaltszeit', 'time_since_last_station': 'Fahrtzeit letzter Halt',\n",
    "                                  'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'delta_lon': 'Breitengrad Ãnderung', 'delta_lat': 'LÃ¤ngengrad Ãnderung'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imp = imp_df(X_train.rename(columns=rename).columns, models['RandomForest'].feature_importances_)\n",
    "base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_plot(base_imp, 'Feature Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(models['RandomForest'], X_test, y_test, n_repeats=10,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "sorted_idx = result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (11.7, 6)\n",
    "fig, ax = plt.subplots(figsize=a4_dims)\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.rename(columns=rename).columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}