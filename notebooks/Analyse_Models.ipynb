{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X, y, X_test, y_test, model, save = False):\n",
    "    start_time = time.time()\n",
    "    #fit model\n",
    "    model.fit(X, y)\n",
    "    #make and print some acc statistics\n",
    "    test_pred = model.predict(X_test)\n",
    "      \n",
    "    acc = test_pred[test_pred == y_test]\n",
    "    \n",
    "    print(1 - ((y_test != test_pred).sum() / X_test.shape[0]))\n",
    "    print('Feature importances', model.feature_importances_)\n",
    "    print('acc: ' + str(len(acc)/ len(test_pred)))\n",
    "    print(accuracy_score(y_test, test_pred))\n",
    "    #print(\"Confusion Matrix: \" + str(confusion_matrix(y_test, test_pred, labels=[0,1])))\n",
    "    print('Confusion Matrix:', multilabel_confusion_matrix(y_test, test_pred))\n",
    "    if save:\n",
    "        dump(model, '../server/test_rf.joblib')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_test, y_test = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dm = DummyClassifier(strategy=\"most_frequent\")\n",
    "dm.fit(X, y)\n",
    "test_pred = dm.predict(X_test)\n",
    "acc = test_pred[test_pred == y_test]\n",
    "    \n",
    "print(\"BASEÂ§LINE: \"+ str(len(acc)/ len(test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model_dt = ExtraTreesClassifier(n_estimators=70, max_depth=12,\n",
    "                                 random_state=0, n_jobs = -1)\n",
    "train_and_test(X, y.ravel(), X_test, y_test.ravel(), model_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_dt = RandomForestClassifier(n_estimators=70, max_depth=12,\n",
    "                                 random_state=0, n_jobs = -1)\n",
    "train_and_test(X, y, X_test, y_test, model_dt, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From here on automatic models for poster"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "'''Change Size of jupyter notebook'''\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  'data/combinedData/alltrains2019.csv'\n",
    "dataset = pd.read_csv(path, index_col=False, compression='zip', engine='c')\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Dates"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['temperature_c',\n",
    "       'air_pressure_hpa', 'relative_humidity', 'dew_point_c',\n",
    "       'wind_speed_kmh', 'weather_condition'], inplace = True, axis=1)\n",
    "date = dataset['date'].astype('datetime64[D]')\n",
    "dataset['month'] = date.dt.month\n",
    "dataset['dayofweek'] = date.dt.dayofweek\n",
    "dataset['hour'] = dataset['zeit']\n",
    "dataset = dataset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(dset, label, random_state):\n",
    "    # make the balance of delayed trains in the dataset better\n",
    "    #split dataset\n",
    "    minor = dset[dataset[label] == True] \n",
    "    major = dset[dataset[label] == False]\n",
    "    #set major dataset to lenght of minor dataset by randomly seletcting datapoints\n",
    "    major = major.sample(n=len(minor),random_state=random_state)\n",
    "    #combine datsets\n",
    "    balancedset = pd.concat([minor, major], ignore_index=True, sort=False)\n",
    "    #I think this shuffels? and ensure length\n",
    "    balancedset = balancedset.sample(n=len(balancedset),random_state=random_state).reset_index(drop=True)\n",
    "    #print(len(balancedset))\n",
    "    return balancedset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = random.randint(1, 1000)\n",
    "label = 'isadelay'\n",
    "feat_labels = [ 'month',\n",
    "                'dayofweek',\n",
    "                'hour',\n",
    "                   'time_since_first_station',\n",
    "                'station_number',\n",
    "                'lat',\n",
    "                'lon',\n",
    "                'stay_time',\n",
    "                'time_since_last_station',\n",
    "                'total_time',\n",
    "                'delta_lon',\n",
    "                'delta_lat'\n",
    "                  ]\n",
    "                # 'relative_humidity', 'dew_point_c', 'air_pressure_hpa', 'temperature_c', 'trainno', 'weather_condition', 'type', 'bhf', 'wind_speed_kmh',\n",
    "df = dataset.sample(frac=1,random_state=random_state)\n",
    "X = dataset[feat_labels]\n",
    "y = dataset['adelay'] > 5 #dataset[label]\n",
    "del df\n",
    "print('SplitDataset')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
    "\n",
    "#and now with balanced\n",
    "dataset_bal = balance(dataset, label, random_state)\n",
    "X_bal = dataset_bal[feat_labels]\n",
    "y_bal = dataset_bal[label]\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_bal, y_bal, test_size=0.1, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Held out something"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def held_out_label(df, feat_labels, label, held_out,held_out_var):\n",
    "    X = df[feat_labels]\n",
    "    y = df[label]\n",
    "    X_train = df[df[held_out_var] != held_out][feat_labels]\n",
    "    X_test = df[df[held_out_var] == held_out][feat_labels]\n",
    "    y_train = df[df[held_out_var] != held_out][label]\n",
    "    y_test = df[df[held_out_var] == held_out][label]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51.517899 = Dortmund Hbf\n",
    "held_out = 51.517899\n",
    "held_out_var = 'lat'\n",
    "X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, held_out, held_out_var)\n",
    "X_train_bal, X_test_bal, y_train_bal,y_test_bal = held_out_label(dataset_bal, feat_labels, label, held_out, held_out_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def plot_roc_auc(actual, preds):\n",
    "    fpr, tpr, thresholds = roc_curve(actual, preds[:,1])\n",
    "    plt.plot(fpr, tpr ,'r')\n",
    "    plt.plot([0,1],[0,1],'b')\n",
    "    plt.xlabel(\"False Posetive\")\n",
    "    plt.ylabel(\"True Posetive\")\n",
    "    plt.title('AUC: {}'.format(auc(fpr,tpr)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "'''Define your models here'''\n",
    "models = {}\n",
    "#models['RandomForest'] = RandomForestClassifier(n_estimators=len(feat_labels)-2, max_depth=12, n_jobs=-1,random_state=random_state),\n",
    "models['DecisionTree'] = DecisionTreeClassifier(max_depth = 12, random_state=random_state)\n",
    "models['RandomForest'] = RandomForestClassifier(n_estimators=64, max_depth = 12, n_jobs=-1,random_state=random_state)\n",
    "models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=64,max_depth = 12, n_jobs=-1,random_state=random_state)\n",
    "\n",
    "'''Calculate Baseline Infos'''\n",
    "zeroR = DummyClassifier(strategy=\"most_frequent\").fit( X_train,y_train).predict(X_test)\n",
    "zeroR_bal = DummyClassifier(strategy=\"most_frequent\").fit( X_train_bal,y_train_bal).predict(X_test_bal)\n",
    "infos = {'random_state': random_state,\n",
    "         'ZeroR': (1 - ((y_test != zeroR).sum() / X_test.shape[0])),\n",
    "         'ZeroR_bal': (1 - ((y_test_bal != zeroR_bal).sum() / X_test_bal.shape[0]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model)\n",
    "    \n",
    "    if False:\n",
    "        scores = cross_val_score(models[model], X, y, cv=StratifiedKFold(n_splits=5))\n",
    "        infos[model + \"_cross_val_strat\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "        \n",
    "    t0 = time.time()    \n",
    "    scores = cross_val_score(models[model], X, y, cv=5)    \n",
    "    infos[model + \"_cross_val\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "    print(scores)\n",
    "    models[model].fit(X_train ,y_train)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(t1-t0)\n",
    "    \n",
    "    model_classified = models[model].predict(X_test)\n",
    "    \n",
    "    infos[model] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "\n",
    "    infos[model + '_matrix'] = confusion_matrix(y_test, model_classified, labels=[0,1])\n",
    "    \n",
    "\n",
    "    '''Balanced'''\n",
    "    \n",
    "    if False:\n",
    "        scores = cross_val_score(models[model], X_bal, y_bal, cv=StratifiedKFold(n_splits=5))\n",
    "        infos[model + \"_cross_val_strat_bal\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "        \n",
    "        scores = cross_val_score(models[model], X_bal, y_bal, cv=5)    \n",
    "        infos[model + \"_cross_val_bal\"] = \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)\n",
    "    \n",
    "    models[model].fit(X_train_bal ,y_train_bal)\n",
    "    model_classified = models[model].predict(X_test_bal)\n",
    "    infos[model + \"_bal\"] = 1 - ((y_test_bal != model_classified).sum() / X_test_bal.shape[0])\n",
    "\n",
    "    infos[model + '_matrix_bal'] = confusion_matrix(y_test_bal, model_classified, labels=[0,1])\n",
    "    \n",
    "    #for feature in zip(feat_labels, models[model].feature_importances_):\n",
    "    #   print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc(y_test, DummyClassifier(strategy=\"most_frequent\").fit( X_train,y_train).predict_proba(X_test))\n",
    "plot_roc_auc(y_test, models['RandomForest'].predict_proba(X_test))\n",
    "plot_roc_auc(y_test, models['ExtraTrees'].predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['train_acc']={}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for train in dataset['trainno'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'trainno')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['train_acc'][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "    \n",
    "    #infos[train + '_matrix'] = confusion_matrix(y_test, model_classified, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['train_acc']={}\n",
    "train_base={}\n",
    "for train in dataset['trainno'].unique():\n",
    "    print(train,end=', ')\n",
    "    test = df[df[held_out_var] == held_out]\n",
    "    train_base[train] = len(test[test['isadelay5'] == True]) / len(test[test['isadelay5'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(train_acc.values())) / len(list(train_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(train_base.values())) / len(list(train_base.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['trainno'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(list(train_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos['bhf_acc']={}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for train in dataset['bhf'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'bhf')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['bhf_acc'][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(list(bhf_acc.values())) / len(list(bhf_acc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['month',\n",
    "                'dayofweek',\n",
    "                'hour',\n",
    "                'time_since_first_station',\n",
    "                'station_number',\n",
    "                'lat',\n",
    "                'lon',\n",
    "                'stay_time',\n",
    "                'time_since_last_station',\n",
    "                'total_time',\n",
    "                'delta_lon',\n",
    "                'delta_lat',\n",
    "                'start_lat',\n",
    "                'start_lon',\n",
    "                'destination_lat',\n",
    "                'destination_lon',\n",
    "                ]].rename(columns={'month': 'Monat', 'dayofweek': 'Wochentag', 'hour': 'Uhrzeit', 'time_since_first_station': 'Gesamte Fahrzeit', 'station_number': 'Anzahl Halte',\n",
    "                                  'lat': 'Breitengrad', 'lon': 'LÃ¤ngengrad', 'lon': 'LÃ¤ngengrad', 'stay_time': 'Aufenthaltszeit', 'time_since_last_station': 'Fahrtzeit letzter Halt',\n",
    "                                  'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'delta_lon': 'Breitengrad Ãnderung', 'delta_lat': 'LÃ¤ngengrad Ãnderung'}).corr().style.background_gradient(cmap='coolwarm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,\n",
    "                   'min_samples_split': 5}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for label, color, setting in [('No shrinkage', 'orange',\n",
    "                               {'learning_rate': 1.0, 'subsample': 1.0}),\n",
    "                              ('learning_rate=0.1', 'turquoise',\n",
    "                               {'learning_rate': 0.1, 'subsample': 1.0}),\n",
    "                              ('subsample=0.5', 'blue',\n",
    "                               {'learning_rate': 1.0, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, subsample=0.5', 'gray',\n",
    "                               {'learning_rate': 0.1, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, max_features=2', 'magenta',\n",
    "                               {'learning_rate': 0.1, 'max_features': 2})]:\n",
    "    params = dict(original_params)\n",
    "    params.update(setting)\n",
    "\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(1 - ((y_test != clf.predict(X_test)).sum() / X_test.shape[0]))\n",
    "    # compute test set deviance\n",
    "    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "        # clf.loss_ assumes that y_test[i] in {0, 1}\n",
    "        test_deviance[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n",
    "            '-', color=color, label=label)\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Test Set Deviance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# udfs ----\n",
    "\n",
    "# function for creating a feature importance dataframe\n",
    "def imp_df(column_names, importances):\n",
    "    df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title):\n",
    "    imp_df.columns = ['feature', 'feature_importance']\n",
    "    a4_dims = (11.7, 6)\n",
    "    fig, ax = plt.subplots(figsize=a4_dims)\n",
    "    sns.barplot(ax=ax ,x = 'feature_importance', y = 'feature', data = imp_df, orient = 'h', color = 'royalblue') \\\n",
    "       .set_title(title, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_imp = imp_df(X_train.rename(columns={'month': 'Monat', 'dayofweek': 'Wochentag', 'hour': 'Uhrzeit', 'time_since_first_station': 'Gesamte Fahrzeit', 'station_number': 'Anzahl Halte',\n",
    "                                  'lat': 'Breitengrad', 'lon': 'LÃ¤ngengrad', 'lon': 'LÃ¤ngengrad', 'stay_time': 'Aufenthaltszeit', 'time_since_last_station': 'Fahrtzeit letzter Halt',\n",
    "                                  'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'total_time': 'PlanmÃ¤Ãige Fahrzeit', 'delta_lon': 'Breitengrad Ãnderung', 'delta_lat': 'LÃ¤ngengrad Ãnderung'}).columns, models['RandomForest'].feature_importances_)\n",
    "base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp_plot(base_imp, 'Feature Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "def drop_col_feat_imp(model, X_train, y_train, random_state = 42):\n",
    "    \n",
    "    # clone the model to have the exact same specification as the one initially trained\n",
    "    model_clone = clone(model)\n",
    "    # set random_state for comparability\n",
    "    model_clone.random_state = random_state\n",
    "    # training and scoring the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    # list for storing feature importances\n",
    "    importances = []\n",
    "    \n",
    "    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n",
    "    for col in X_train.columns:\n",
    "        print(col)\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "    \n",
    "    importances_df = imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_imp = drop_col_feat_imp(models['RandomForest'], X_train, y_train)\n",
    "var_imp_plot(drop_imp, 'Drop Column feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_imp = drop_col_feat_imp(models['RandomForest'], X_train, y_train)\n",
    "var_imp_plot(drop_imp, 'Drop Column feature importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "print(\"Writeing to file #1\")\n",
    "print(infos)\n",
    "#with open('info.json', 'w') as fp:\n",
    "#   json.dump(infos, fp)\n",
    "print(\"Done\")\n",
    "infos['train_acc']={}\n",
    "for model in models:\n",
    "    print(\"\\n\",)\n",
    "    print(model)\n",
    "    infos['train_acc'][model]={}\n",
    "    for train in dataset['trainno'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'trainno')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['train_acc'][model][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "    print(str(infos))\n",
    "\n",
    "print(\"\\n\\n\\n\\n INFOS:\")\n",
    "print(str(infos))\n",
    "print(\"Done\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "infos['bhf_acc']={}\n",
    "for model in models:\n",
    "    print(\"\\n\")\n",
    "    print(model)\n",
    "    infos['bhf_acc'][model]={}\n",
    "    for train in dataset['bhf'].unique():\n",
    "        print(train,end=', ')\n",
    "        X_train, X_test, y_train, y_test = held_out_label(dataset, feat_labels, label, train, 'bhf')\n",
    "        models[model].fit(X_train ,y_train)\n",
    "        model_classified = models[model].predict(X_test)\n",
    "        infos['bhf_acc'][model][train] = 1 - ((y_test != model_classified).sum() / X_test.shape[0])\n",
    "    print(str(infos))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\\n INFOS:\")\n",
    "print(str(infos))\n",
    "print(\"Done\\n\\n\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}